from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
import tensorflow.keras as keras

# Define your input shape
train_input_shape = (224, 224, 3)

# Assuming n_classes is defined somewhere
n_classes = artists_top.shape[0]

# Load the base ResNet50 model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)

# Model
base_model = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)

for layer in base_model.layers:
    layer.trainable = True

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
predictions = Dense(n_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

optimizer = Adam(learning_rate=0.0001)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

n_epochs = 15

# Learning rate schedule
def lr_schedule(epoch):
    if epoch < 10:
        return 0.0001
    else:
        return 0.0001 * tf.math.exp(0.1 * (10 - epoch))

lr_scheduler = LearningRateScheduler(lr_schedule)

early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1,
                           restore_best_weights=True)

history = model.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    validation_data=valid_generator,
                    validation_steps=len(valid_generator),
                    epochs=n_epochs,
                    callbacks=[lr_scheduler, early_stop],
                    class_weight=class_weights
                   )


# Save the model to a file
model.save('model.h5')
